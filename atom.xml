<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WuHainan&#39;Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wuhainan.com/"/>
  <updated>2018-11-02T08:23:01.172Z</updated>
  <id>http://wuhainan.com/</id>
  
  <author>
    <name>天道酬勤whn</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://wuhainan.com/2018/11/02/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://wuhainan.com/2018/11/02/逻辑回归/</id>
    <published>2018-11-02T07:58:20.000Z</published>
    <updated>2018-11-02T08:23:01.172Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://baidu.com" target="_blank" rel="noopener">百度</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://baidu.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;百度&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="分类" scheme="http://wuhainan.com/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>线性模型</title>
    <link href="http://wuhainan.com/2018/10/29/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://wuhainan.com/2018/10/29/线性模型/</id>
    <published>2018-10-29T12:28:35.000Z</published>
    <updated>2018-11-03T00:25:27.724Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>给定数据集D={$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots(x^{(m)},y^{(m)})$}，$x^{(i)}\in\mathcal{X}\subseteq\mathbb{R}^m,y^{(i)}\in\mathcal{Y}\subseteq\mathbb{R},i=1,2,\cdots,m,$其中$x^{(i)}=(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)})^T,$m为样本的数量，n为特征的数量。</p><p>线性模型试图学得一个通过属性的线性组合来进行预测的函数，即</p><script type="math/tex; mode=display">f(x)=w_1x_1+w_2x_2+...+w_nx_n+b,</script><p>其中$w$为权重，b为截距，写成向量形式为</p><script type="math/tex; mode=display">f(x)=w^Tx+b</script><p>为了简化公式，设$x^{(i)}=(x_1^{(i)},x_2^{(i)},…x_n^{(i)},1)^T，w=(w_1,w_2,…,w_n,b)^T$，简化之后为</p><script type="math/tex; mode=display">f(x)=\sum_{i=0}^nw_ix_i=w^Tx</script><p>我们的目标便是通过给定的数据集D来学习参数$w$，对于给定的样本$x^{(i)}$，其预测值$\hat{y}^{(i)}=w^Tx^{(i)}$，与真实值$y^{(i)}$越接近越好。这里我们采用平方损失函数，则在训练集D上，模型的损失函数为</p><script type="math/tex; mode=display">L(w)=\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2\\\quad\quad=\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2</script><p>这样，我们的目标便变为损失函数最小化。为了之后求导方便，在损失函数前乘以1/2，即：</p><script type="math/tex; mode=display">w^*=\mathop{\arg\min}_{w}\frac{1}{2}\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2</script><p>为了求出使$L(w)$最小的$w$值，我们可以使用梯度下降法和正规方程两种方法。</p><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>梯度下降的思想是：开始时随机选择一个参数的组合$(w_0,w_1,w_2,…,w_n)$，计算损失函数，然后寻找下一个能让损失函数下降最多的参数组合，持续这么做直到一个局部最小值。通常选择不同的初始参数组合，可能会找到不同的局部最小值。</p><p>梯度下降算法公式为：</p><p>重复直到收敛{</p><p>​    $w_j:=w_j-\alpha\frac{\partial}{\partial w_j}L(w)$    </p><p>}</p><p>要实现这个算法，关键在于求出损失函数关于$w$的导数</p><p>$\frac{\partial}{\partial w_j}L(w)=\frac{\partial}{w_j}\frac{1}{2}\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2<br>\\\quad\quad\quad\,\,\,\,=\frac{\partial}{\partial w_j}\frac{1}{2}\sum_{i=1}^m(w_0x_0^{(i)}+w_1x_1^{(i)}+…+w_jx_j^{(i)}+…+w_nx_n^{(i)}-y^{(i)})^2<br>\\\quad\quad\quad\,\,\,\,=2·\frac{1}{2}\sum_{i=1}^m(w_0x_0^{(i)}+w_1x_1^{(i)}+…+w_nx_n^{(i)}-y^{(i)})·x_j^{(i)}<br>\\\quad\quad\quad\,\,\,\,=\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})x_j^{(i)}$</p><p>重复直到收敛{</p><p>​    $w_j:=w_j+\alpha\sum_{i=1}^m(y^{(i)}-w^Tx^{(i)})x_j^{(i)}​$        (for  every  j)</p><p>}</p><h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>正规方程通过求解下面的方程来找出使损失函数最小的参数：$\frac{\partial}{\partial w}L(w)=0$</p><h3 id="矩阵导数"><a href="#矩阵导数" class="headerlink" title="矩阵导数"></a>矩阵导数</h3><p>假设函数$f:R^{m×n}\to R$，从m*n大小的矩阵映射到实数域，那么当矩阵为A时导函数定义如下所示：</p><script type="math/tex; mode=display">\frac{\partial f(A)}{\partial A}=\begin{bmatrix}\frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1n}}\\\vdots &\ddots&\vdots\\\frac{\partial f}{\partial A_{m1}}&\cdots&\frac{\partial f}{\partial A_{mn}} \end{bmatrix}</script><p>例如A=$\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22} \end{bmatrix}$是2*2矩阵，给定函数$f:R^{2×2} \to R$为：</p><script type="math/tex; mode=display">f(A)=\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22}</script><p>那么$\frac{\partial f(A)}{\partial A}=\begin{bmatrix}\frac{3}{2}&amp;10A_{12}\\A_{22}&amp;A_{21} \end{bmatrix}$，我们还要引入矩阵的迹(trace)，简写为tr。对于一个给定的n*n的方阵A，它的迹定义为对角线元素之和：</p><script type="math/tex; mode=display">tr\ A=\sum_{i=1}^nA_{ii}</script><p>如果有两矩阵A和B，满足AB为方阵，则迹运算有以下性质：</p><script type="math/tex; mode=display">trAB=trBA\\trABC=trCAB=trBCA\\trA=trA^T\\tr(A+B)=trA+trB\\tr\ aA=atrA</script><p>接下来提出一些矩阵导数：</p><script type="math/tex; mode=display">\frac{\partial(trAB)}{\partial A}=B^T\\\frac{\partial f(A)}{\partial A^T}=(\frac{\partial f(A)}{\partial A})^T\\\frac{\partial(trABA^TC)}{\partial A}=CAB+C^TAB^T\\\frac{\partial |A|}{\partial A}=|A|(A^{-1})^T</script><p>下面把损失函数$L(w)$用向量的形式表述。令</p><script type="math/tex; mode=display">X=\begin{bmatrix}(x^{(1)})^T\\(x^{(2)})^T\\\vdots\\(x^{(m)})^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)}&x_2^{(1)}&\cdots& x_n^{(1)}&1\\x_1^{(2)}&x_2^{(2)}&\cdots&x_n^{(2)}&1\\\vdots&\vdots&\ddots&\vdots&1\\x_1^{(m)}&x_2^{(m)}&\cdots&x_n^{(m)}&1\end{bmatrix}\\y=\begin{bmatrix}y^{(1)}\\y^{(2)}\\\vdots\\y^{(m)}\end{bmatrix},w=\begin{bmatrix}w_1\\w_2\\\vdots\\w_n\\b\end{bmatrix}</script><p>则有</p><script type="math/tex; mode=display">Xw-y=\begin{bmatrix}f(x^{(1)})-y^{(1)}\\f(x^{(2)})-y^{(2)}\\\vdots\\f(x^{(m)})-y^{(m)}\end{bmatrix}\\\frac{1}{2}(Xw-y)^T(Xw-y)=\frac{1}{2}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2=L(w)</script><script type="math/tex; mode=display">\begin{equation}\begin{split}\frac{\partial L(w)}{\partial w}&=\frac{\partial }{\partial w}\frac{1}{2}(Xw-y)^T(Xw-y)\\&=\frac{1}{2}\frac{\partial}{\partial w}(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty)\\&=\frac{1}{2}\frac{\partial}{\partial w}tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty)\\&=\frac{1}{2}\frac{\partial}{\partial w}(tr(w^TX^TXw)-2tr(y^TXw))\\&=\frac{1}{2}(X^TXw+X^TXw-2X^Ty)\\&=X^TXw-X^Ty\end{split}\end{equation}</script><p>令其等于0便得到下面的正规方程：</p><script type="math/tex; mode=display">X^TXw=X^Ty</script><p>当$X^TX$可逆时，可得：</p><script type="math/tex; mode=display">w^*=(X^TX)^{-1}X^Ty</script><p>于是学得的线性回归模型为：</p><script type="math/tex; mode=display">f(x^{(i)})=w^{*T}x^{(i)}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性回归模型&quot;&gt;&lt;a href=&quot;#线性回归模型&quot; class=&quot;headerlink&quot; title=&quot;线性回归模型&quot;&gt;&lt;/a&gt;线性回归模型&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wuhainan.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wuhainan.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>k-近邻</title>
    <link href="http://wuhainan.com/2018/10/16/k-%E8%BF%91%E9%82%BB/"/>
    <id>http://wuhainan.com/2018/10/16/k-近邻/</id>
    <published>2018-10-16T07:34:15.000Z</published>
    <updated>2018-11-06T08:14:09.774Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>k近邻法是一种基本的分类与回归算法，它的输入为实例的特征向量，通过计算新数据与训练数据特征值之间的距离，然后选取与该实例最邻近的k个实例，这k个实例的多数属于哪一类，就把该输入实例分到这个类。对于分类问题，输出为实例的类别，对于新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测；对于回归问题，输出为实例的值，对于新的实例，取其k个最近邻的训练实例的平均值作为预测值。</p><h1 id="kNN三要素"><a href="#kNN三要素" class="headerlink" title="kNN三要素"></a>kNN三要素</h1><p>k近邻法不具有显式的学习过程，它是直接预测，实际上是利用训练数据集对特征向量空间划分，作为其分类的模型，模型由距离度量、k值的选择、分类决策规则三要素决定。</p><h2 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h2><p>特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是n维实数向量空间$\mathbb{R}^n$。k近邻模型的特征空间的距离一般使用欧氏距离，也可以是更一般的$L_p$距离。</p><p>设特征空间$\mathcal{X}$是n维实数向量空间，$x_i,x_j\in\mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T,x_i,x_j$的$L_p$距离定义为：</p><script type="math/tex; mode=display">L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p}</script><p>这里$p\ge1$，当$p=2$时，为欧氏距离：</p><script type="math/tex; mode=display">L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^\frac{1}{2}</script><p>当$p=1$时，为曼哈顿距离：</p><script type="math/tex; mode=display">L_1(x_i,x_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|</script><p>当$p=\infty$时，为各个维度距离的最大值：</p><script type="math/tex; mode=display">L_\infty(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|</script><h2 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h2><p>如果选择较小的k值，相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。即k值的减小意味着整体模型变得复杂，容易发生过拟合。</p><p>如果选择较大的k值，相当于用较大的邻域中的训练实例进行预测，其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误，即k值的增大意味着整体模型变得简单。当k=N时，无论输入实例是什么，都将简单地预测它为训练实例中最多的类。此时，模型过于简单，完全忽略训练实例中的大量有用信息。</p><p>在应用中，k一般取一个较小的数值。通常采用交叉验证法选取最优的k值，就是比较不同k值时的交叉验证平均误差率，选择误差率最小的那个k值。</p><h2 id="分类决策规则"><a href="#分类决策规则" class="headerlink" title="分类决策规则"></a>分类决策规则</h2><h2 id="sklearn代码"><a href="#sklearn代码" class="headerlink" title="#sklearn代码"></a>#sklearn代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Oct 10 15:59:12 2018</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: wuhainan</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir  <span class="comment">#列出给定目录的文件名</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group=array([[<span class="number">1.0</span>,<span class="number">1.1</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]])</span><br><span class="line">    labels=[<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group,labels</span><br><span class="line">    </span><br><span class="line"><span class="comment">#k近邻算法 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX,dataSet,labels,k)</span>:</span> <span class="comment">#inX为用于分类的输入向量,dataSet为训练样本集,labels为标签向量 </span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    diffMat = tile(inX,(dataSetSize,<span class="number">1</span>)) - dataSet <span class="comment">#tile函数将inX重复为与dataSet相同的行数</span></span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances = sqDistances**<span class="number">0.5</span>  <span class="comment">#inX到每个实例点的距离</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>)+<span class="number">1</span>  <span class="comment">#如果该类别不存在，初值置为0，已存在则加1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),    <span class="comment">#选出距离最小的k个实例，按标签出现的次数从大到小排序</span></span><br><span class="line">                              key = operator.itemgetter(<span class="number">1</span>),reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#--------------------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">#使用k-近邻算法改进约会网站的配对效果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span>  <span class="comment">#将文本文件转换为列表</span></span><br><span class="line">    fr=open(filename)   <span class="comment">#读取文件</span></span><br><span class="line">    arrayOLines=fr.readlines()  <span class="comment">#返回由文件的每一行为元素构成的列表</span></span><br><span class="line">    numberOfLines=len(arrayOLines)  <span class="comment">#文件的总行数</span></span><br><span class="line">    returnMat=zeros((numberOfLines,<span class="number">3</span>))  <span class="comment">#特征向量</span></span><br><span class="line">    classLabelVector=[]  <span class="comment">#标签向量</span></span><br><span class="line">    index=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> arrayOLines:  <span class="comment">#依次读取每行</span></span><br><span class="line">        line=line.strip()  <span class="comment">#去掉每行头尾空白</span></span><br><span class="line">        listFromLine=line.split(<span class="string">'\t'</span>) <span class="comment">#返回以制表符\t分隔之后的列表，如['40920', '8.326976', '0.953952', 'largeDoses']</span></span><br><span class="line">        returnMat[index,:]=listFromLine[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">        classLabelVector.append(int(listFromLine[<span class="number">-1</span>]))</span><br><span class="line">        index+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnMat,classLabelVector</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span>  <span class="comment">#归一化特征值</span></span><br><span class="line">    minVals=dataSet.min(<span class="number">0</span>)  <span class="comment">#每一列的最小值</span></span><br><span class="line">    maxVals=dataSet.max(<span class="number">0</span>)  <span class="comment">#每一列的最大值</span></span><br><span class="line">    ranges=maxVals-minVals</span><br><span class="line">    normalDataSet=zeros(shape(dataSet))</span><br><span class="line">    m=dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    normalDataSet=dataSet-tile(minVals,(m,<span class="number">1</span>))</span><br><span class="line">    normalDataSet=normalDataSet/tile(ranges,(m,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> normalDataSet,ranges,minVals</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hoRatio=<span class="number">0.10</span></span><br><span class="line">    datingDataMat,datingLabels=file2matrix(<span class="string">"datingTestSet2.txt"</span>)</span><br><span class="line">    normalMat,ranges,minVals=autoNorm(datingDataMat)</span><br><span class="line">    m=normalMat.shape[<span class="number">0</span>]  <span class="comment">#总样本数量</span></span><br><span class="line">    numTestVecs=int(m*hoRatio)  <span class="comment">#测试集数量</span></span><br><span class="line">    errorCount=<span class="number">0.0</span>  <span class="comment">#用于错误结果计数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">        classifierResult=classify0(normalMat[i,:],normalMat[numTestVecs:m,:],</span><br><span class="line">                                   datingLabels[numTestVecs:m],<span class="number">3</span>)</span><br><span class="line">        print(<span class="string">"the classifier came back with: %d,the real answer is %d"</span> % (classifierResult,datingLabels[i]))</span><br><span class="line">        <span class="keyword">if</span>(classifierResult != datingLabels[i]):</span><br><span class="line">            errorCount+=<span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"the total error rate is %f"</span> % (errorCount/float(numTestVecs)))</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyPerson</span><span class="params">()</span>:</span>  <span class="comment">#预测函数</span></span><br><span class="line">    resultList=[<span class="string">'not at all'</span>,<span class="string">'in small doses'</span>,<span class="string">'in large doses'</span>]</span><br><span class="line">    percentTats=float(input(<span class="string">"percentage of time spent playing video game?"</span>))  <span class="comment">#玩视频游戏所耗时间百分比</span></span><br><span class="line">    ffMiles=float(input(<span class="string">"frequent flier miles earned per year?"</span>))  <span class="comment">#每年获得的飞行常客里程数</span></span><br><span class="line">    iceCream=float(input(<span class="string">"liters of ice cream consumed per year?"</span>))  <span class="comment">#每周消费的冰淇淋公升数</span></span><br><span class="line">    datingDataMat,datingLabels=file2matrix(<span class="string">"datingTestSet2.txt"</span>)</span><br><span class="line">    normalMat,ranges,minVals=autoNorm(datingDataMat)</span><br><span class="line">    inArr=array([ffMiles,percentTats,iceCream])</span><br><span class="line">    classifierResult=classify0((inArr-minVals)/ranges,normalMat,datingLabels,<span class="number">3</span>)</span><br><span class="line">    print(<span class="string">"You will probably like this person:"</span>,resultList[classifierResult<span class="number">-1</span>])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">#使用k-近邻算法识别手写数字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span>  <span class="comment">#将图像转换为1×1024的向量</span></span><br><span class="line">    returnVector=zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr=open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        lineStr=fr.readline()  <span class="comment">#读取每一行</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            returnVector[<span class="number">0</span>,<span class="number">32</span>*i+j]=int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVector</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hwLabels=[]  <span class="comment">#标签向量</span></span><br><span class="line">    trainFileList=listdir(<span class="string">"trainingDigits"</span>)  <span class="comment">#返回以trainingDigits目录下的所有文件名为元素的列表</span></span><br><span class="line">    m=len(trainFileList)  <span class="comment">#样本数量</span></span><br><span class="line">    trainingMat=zeros((m,<span class="number">1024</span>))  <span class="comment">#训练集特征向量,每一行存储一个数字的图像信息</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fileNameStr=trainFileList[i]</span><br><span class="line">        fileStr=fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        classNumStr=int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])  <span class="comment">#从文件名解析出具体的数字</span></span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        trainingMat[i,:]=img2vector(<span class="string">'trainingDigits/%s'</span> % fileNameStr)  <span class="comment">#将该样本添加到训练集中</span></span><br><span class="line">    testFileList=listdir(<span class="string">"testDigits"</span>)</span><br><span class="line">    mTest=len(testFileList)  <span class="comment">#测试样本数量</span></span><br><span class="line">    errorCount=<span class="number">0.0</span>  <span class="comment">#记录误分类样本数量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</span><br><span class="line">        fileNameStr=testFileList[i]</span><br><span class="line">        fileStr=fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        classNumStr=int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest=img2vector(<span class="string">"testDigits/%s"</span> % fileNameStr)  <span class="comment">#测试向量</span></span><br><span class="line">        classifierResult=classify0(vectorUnderTest,trainingMat,hwLabels,<span class="number">3</span>)</span><br><span class="line">        print(<span class="string">"the classifier came back with: %d,the real answer is %d"</span> % (classifierResult,classNumStr))</span><br><span class="line">        <span class="keyword">if</span>(classifierResult != classNumStr):</span><br><span class="line">            errorCount+=<span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"the total number of errors is: %d"</span> % errorCount)  <span class="comment">#误分类样本总数量</span></span><br><span class="line">    print(<span class="string">"the total error rate is %f"</span> % (errorCount/float(mTest)))  <span class="comment">#错误率</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;k近邻法是一种基本的分类与回归算法，它的输入为实例的特征向量，通过计算新数据与训练数据特征值之间的距离，然后选取与该实例最邻近的k个实例，这
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wuhainan.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wuhainan.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
