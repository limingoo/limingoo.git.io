<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2018%2F11%2F02%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[百度]]></content>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性模型]]></title>
    <url>%2F2018%2F10%2F29%2F%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[线性回归模型概述给定数据集D={$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots(x^{(m)},y^{(m)})$}，$x^{(i)}\in\mathcal{X}\subseteq\mathbb{R}^m,y^{(i)}\in\mathcal{Y}\subseteq\mathbb{R},i=1,2,\cdots,m,$其中$x^{(i)}=(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)})^T,$m为样本的数量，n为特征的数量。 线性模型试图学得一个通过属性的线性组合来进行预测的函数，即 f(x)=w_1x_1+w_2x_2+...+w_nx_n+b,其中$w$为权重，b为截距，写成向量形式为 f(x)=w^Tx+b为了简化公式，设$x^{(i)}=(x_1^{(i)},x_2^{(i)},…x_n^{(i)},1)^T，w=(w_1,w_2,…,w_n,b)^T$，简化之后为 f(x)=\sum_{i=0}^nw_ix_i=w^Tx我们的目标便是通过给定的数据集D来学习参数$w$，对于给定的样本$x^{(i)}$，其预测值$\hat{y}^{(i)}=w^Tx^{(i)}$，与真实值$y^{(i)}$越接近越好。这里我们采用平方损失函数，则在训练集D上，模型的损失函数为 L(w)=\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2 \\\quad\quad=\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2这样，我们的目标便变为损失函数最小化。为了之后求导方便，在损失函数前乘以1/2，即： w^*=\mathop{\arg\min}_{w}\frac{1}{2}\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2为了求出使$L(w)$最小的$w$值，我们可以使用梯度下降法和正规方程两种方法。 梯度下降法梯度下降的思想是：开始时随机选择一个参数的组合$(w_0,w_1,w_2,…,w_n)$，计算损失函数，然后寻找下一个能让损失函数下降最多的参数组合，持续这么做直到一个局部最小值。通常选择不同的初始参数组合，可能会找到不同的局部最小值。 梯度下降算法公式为： 重复直到收敛{ ​ $w_j:=w_j-\alpha\frac{\partial}{\partial w_j}L(w)$ } 要实现这个算法，关键在于求出损失函数关于$w$的导数 $\frac{\partial}{\partial w_j}L(w)=\frac{\partial}{w_j}\frac{1}{2}\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2\\\quad\quad\quad\,\,\,\,=\frac{\partial}{\partial w_j}\frac{1}{2}\sum_{i=1}^m(w_0x_0^{(i)}+w_1x_1^{(i)}+…+w_jx_j^{(i)}+…+w_nx_n^{(i)}-y^{(i)})^2\\\quad\quad\quad\,\,\,\,=2·\frac{1}{2}\sum_{i=1}^m(w_0x_0^{(i)}+w_1x_1^{(i)}+…+w_nx_n^{(i)}-y^{(i)})·x_j^{(i)}\\\quad\quad\quad\,\,\,\,=\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})x_j^{(i)}$ 重复直到收敛{ ​ $w_j:=w_j+\alpha\sum_{i=1}^m(y^{(i)}-w^Tx^{(i)})x_j^{(i)}​$ (for every j) } 正规方程正规方程通过求解下面的方程来找出使损失函数最小的参数：$\frac{\partial}{\partial w}L(w)=0$ 矩阵导数假设函数$f:R^{m×n}\to R$，从m*n大小的矩阵映射到实数域，那么当矩阵为A时导函数定义如下所示： \frac{\partial f(A)}{\partial A}=\begin{bmatrix}\frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1n}}\\\vdots &\ddots&\vdots\\\frac{\partial f}{\partial A_{m1}}&\cdots&\frac{\partial f}{\partial A_{mn}} \end{bmatrix}例如A=$\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22} \end{bmatrix}$是2*2矩阵，给定函数$f:R^{2×2} \to R$为： f(A)=\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22}那么$\frac{\partial f(A)}{\partial A}=\begin{bmatrix}\frac{3}{2}&amp;10A_{12}\\A_{22}&amp;A_{21} \end{bmatrix}$，我们还要引入矩阵的迹(trace)，简写为tr。对于一个给定的n*n的方阵A，它的迹定义为对角线元素之和： tr\ A=\sum_{i=1}^nA_{ii}如果有两矩阵A和B，满足AB为方阵，则迹运算有以下性质： trAB=trBA\\trABC=trCAB=trBCA\\trA=trA^T\\tr(A+B)=trA+trB\\tr\ aA=atrA接下来提出一些矩阵导数： \frac{\partial(trAB)}{\partial A}=B^T\\\frac{\partial f(A)}{\partial A^T}=(\frac{\partial f(A)}{\partial A})^T\\\frac{\partial(trABA^TC)}{\partial A}=CAB+C^TAB^T\\\frac{\partial |A|}{\partial A}=|A|(A^{-1})^T下面把损失函数$L(w)$用向量的形式表述。令 X=\begin{bmatrix}(x^{(1)})^T\\(x^{(2)})^T\\\vdots\\(x^{(m)})^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)}&x_2^{(1)}&\cdots& x_n^{(1)}&1\\x_1^{(2)}&x_2^{(2)}&\cdots&x_n^{(2)}&1\\\vdots&\vdots&\ddots&\vdots&1\\x_1^{(m)}&x_2^{(m)}&\cdots&x_n^{(m)}&1\end{bmatrix}\\y=\begin{bmatrix}y^{(1)}\\y^{(2)}\\\vdots\\y^{(m)}\end{bmatrix},w=\begin{bmatrix}w_1\\w_2\\\vdots\\w_n\\b\end{bmatrix}则有 Xw-y=\begin{bmatrix}f(x^{(1)})-y^{(1)}\\f(x^{(2)})-y^{(2)}\\\vdots\\f(x^{(m)})-y^{(m)}\end{bmatrix}\\\frac{1}{2}(Xw-y)^T(Xw-y)=\frac{1}{2}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2=L(w) \begin{equation} \begin{split} \frac{\partial L(w)}{\partial w}&=\frac{\partial }{\partial w}\frac{1}{2}(Xw-y)^T(Xw-y)\\ &=\frac{1}{2}\frac{\partial}{\partial w}(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty)\\ &=\frac{1}{2}\frac{\partial}{\partial w}tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty)\\ &=\frac{1}{2}\frac{\partial}{\partial w}(tr(w^TX^TXw)-2tr(y^TXw))\\ &=\frac{1}{2}(X^TXw+X^TXw-2X^Ty)\\ &=X^TXw-X^Ty \end{split} \end{equation}令其等于0便得到下面的正规方程： X^TXw=X^Ty当$X^TX$可逆时，可得： w^*=(X^TX)^{-1}X^Ty于是学得的线性回归模型为： f(x^{(i)})=w^{*T}x^{(i)}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻]]></title>
    <url>%2F2018%2F10%2F16%2Fk-%E8%BF%91%E9%82%BB%2F</url>
    <content type="text"><![CDATA[概述k近邻法是一种基本的分类与回归算法，它的输入为实例的特征向量，通过计算新数据与训练数据特征值之间的距离，然后选取与该实例最邻近的k个实例，这k个实例的多数属于哪一类，就把该输入实例分到这个类。对于分类问题，输出为实例的类别，对于新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测；对于回归问题，输出为实例的值，对于新的实例，取其k个最近邻的训练实例的平均值作为预测值。 kNN三要素k近邻法不具有显式的学习过程，它是直接预测，实际上是利用训练数据集对特征向量空间划分，作为其分类的模型，模型由距离度量、k值的选择、分类决策规则三要素决定。 距离度量特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是n维实数向量空间$\mathbb{R}^n$。k近邻模型的特征空间的距离一般使用欧氏距离，也可以是更一般的$L_p$距离。 设特征空间$\mathcal{X}$是n维实数向量空间，$x_i,x_j\in\mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T,x_i,x_j$的$L_p$距离定义为： L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p}这里$p\ge1$，当$p=2$时，为欧氏距离： L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^\frac{1}{2}当$p=1$时，为曼哈顿距离： L_1(x_i,x_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|当$p=\infty$时，为各个维度距离的最大值： L_\infty(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|k值的选择如果选择较小的k值，相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。即k值的减小意味着整体模型变得复杂，容易发生过拟合。 如果选择较大的k值，相当于用较大的邻域中的训练实例进行预测，其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误，即k值的增大意味着整体模型变得简单。当k=N时，无论输入实例是什么，都将简单地预测它为训练实例中最多的类。此时，模型过于简单，完全忽略训练实例中的大量有用信息。 在应用中，k一般取一个较小的数值。通常采用交叉验证法选取最优的k值，就是比较不同k值时的交叉验证平均误差率，选择误差率最小的那个k值。 分类决策规则sklearn代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# -*- coding: utf-8 -*-"""Created on Wed Oct 10 15:59:12 2018@author: wuhainan"""from numpy import *import operatorfrom os import listdir #列出给定目录的文件名def createDataSet(): group=array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels=['A','A','B','B'] return group,labels #k近邻算法 def classify0(inX,dataSet,labels,k): #inX为用于分类的输入向量,dataSet为训练样本集,labels为标签向量 dataSetSize = dataSet.shape[0] diffMat = tile(inX,(dataSetSize,1)) - dataSet #tile函数将inX重复为与dataSet相同的行数 sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 #inX到每个实例点的距离 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0)+1 #如果该类别不存在，初值置为0，已存在则加1 sortedClassCount = sorted(classCount.items(), #选出距离最小的k个实例，按标签出现的次数从大到小排序 key = operator.itemgetter(1),reverse=True) return sortedClassCount[0][0]#--------------------------------------------------------------------------------------------------#使用k-近邻算法改进约会网站的配对效果def file2matrix(filename): #将文本文件转换为列表 fr=open(filename) #读取文件 arrayOLines=fr.readlines() #返回由文件的每一行为元素构成的列表 numberOfLines=len(arrayOLines) #文件的总行数 returnMat=zeros((numberOfLines,3)) #特征向量 classLabelVector=[] #标签向量 index=0 for line in arrayOLines: #依次读取每行 line=line.strip() #去掉每行头尾空白 listFromLine=line.split('\t') #返回以制表符\t分隔之后的列表，如['40920', '8.326976', '0.953952', 'largeDoses'] returnMat[index,:]=listFromLine[0:3] classLabelVector.append(int(listFromLine[-1])) index+=1 return returnMat,classLabelVector def autoNorm(dataSet): #归一化特征值 minVals=dataSet.min(0) #每一列的最小值 maxVals=dataSet.max(0) #每一列的最大值 ranges=maxVals-minVals normalDataSet=zeros(shape(dataSet)) m=dataSet.shape[0] normalDataSet=dataSet-tile(minVals,(m,1)) normalDataSet=normalDataSet/tile(ranges,(m,1)) return normalDataSet,ranges,minVals def datingClassTest(): hoRatio=0.10 datingDataMat,datingLabels=file2matrix("datingTestSet2.txt") normalMat,ranges,minVals=autoNorm(datingDataMat) m=normalMat.shape[0] #总样本数量 numTestVecs=int(m*hoRatio) #测试集数量 errorCount=0.0 #用于错误结果计数 for i in range(numTestVecs): classifierResult=classify0(normalMat[i,:],normalMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print("the classifier came back with: %d,the real answer is %d" % (classifierResult,datingLabels[i])) if(classifierResult != datingLabels[i]): errorCount+=1.0 print("the total error rate is %f" % (errorCount/float(numTestVecs))) def classifyPerson(): #预测函数 resultList=['not at all','in small doses','in large doses'] percentTats=float(input("percentage of time spent playing video game?")) #玩视频游戏所耗时间百分比 ffMiles=float(input("frequent flier miles earned per year?")) #每年获得的飞行常客里程数 iceCream=float(input("liters of ice cream consumed per year?")) #每周消费的冰淇淋公升数 datingDataMat,datingLabels=file2matrix("datingTestSet2.txt") normalMat,ranges,minVals=autoNorm(datingDataMat) inArr=array([ffMiles,percentTats,iceCream]) classifierResult=classify0((inArr-minVals)/ranges,normalMat,datingLabels,3) print("You will probably like this person:",resultList[classifierResult-1]) #------------------------------------------------------------------------------------#使用k-近邻算法识别手写数字def img2vector(filename): #将图像转换为1×1024的向量 returnVector=zeros((1,1024)) fr=open(filename) for i in range(32): lineStr=fr.readline() #读取每一行 for j in range(32): returnVector[0,32*i+j]=int(lineStr[j]) return returnVector def handwritingClassTest(): hwLabels=[] #标签向量 trainFileList=listdir("trainingDigits") #返回以trainingDigits目录下的所有文件名为元素的列表 m=len(trainFileList) #样本数量 trainingMat=zeros((m,1024)) #训练集特征向量,每一行存储一个数字的图像信息 for i in range(m): fileNameStr=trainFileList[i] fileStr=fileNameStr.split('.')[0] classNumStr=int(fileStr.split('_')[0]) #从文件名解析出具体的数字 hwLabels.append(classNumStr) trainingMat[i,:]=img2vector('trainingDigits/%s' % fileNameStr) #将该样本添加到训练集中 testFileList=listdir("testDigits") mTest=len(testFileList) #测试样本数量 errorCount=0.0 #记录误分类样本数量 for i in range(mTest): fileNameStr=testFileList[i] fileStr=fileNameStr.split('.')[0] classNumStr=int(fileStr.split('_')[0]) vectorUnderTest=img2vector("testDigits/%s" % fileNameStr) #测试向量 classifierResult=classify0(vectorUnderTest,trainingMat,hwLabels,3) print("the classifier came back with: %d,the real answer is %d" % (classifierResult,classNumStr)) if(classifierResult != classNumStr): errorCount+=1.0 print("the total number of errors is: %d" % errorCount) #误分类样本总数量 print("the total error rate is %f" % (errorCount/float(mTest))) #错误率]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
