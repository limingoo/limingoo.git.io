<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Git学习总结]]></title>
    <url>%2F2019%2F01%2F05%2FGit%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[git基本操作创建版本仓库 git init 版本创建 git add 文件或目录 git commit -m ‘说明信息’ 查看版本记录 git log版本回退 git reset —hard HEAD^ git reset —hard 版本序列号查看操作记录 git reflog工作区、版本库和暂存区 git add是把工作区的修改放入暂存区 git commit是把暂存区的所有修改一次性提交管理修改 git commit只会把暂存区的修改提交到版本记录中撤销修改 直接丢弃工作区的改动 git checkout — 文件名 修改已经添加到暂存区，但未commit a. 首先git reset HEAD 文件名 b. 然后git checkout — 文件名 已经commit 版本回退对比文件的不同对比工作区和版本库的某个文件​ git diff HEAD — 文件名对比两个版本的文件​ git diff HEAD HEAD^ — 文件名删除文件 rm 文件名 git rm 文件名 git commit 分支管理]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一张图总结Git]]></title>
    <url>%2F2019%2F01%2F05%2F%E4%B8%80%E5%BC%A0%E5%9B%BE%E6%80%BB%E7%BB%93Git%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法]]></title>
    <url>%2F2018%2F12%2F26%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%2F</url>
    <content type="text"><![CDATA[方向导数方向导数本质上研究的是函数在某点处沿某特定方向上的变化率问题，即某个方向上的导数。 定义 $z=f(x,y)((x,y)\in D)，M_0(x_0,y_0)\in D，在$$D$中过$M_0$作射线$l$，取$M(x_0+\Delta x，y_0+\Delta y)\in l，\Delta z=f(x_0+\Delta x，y_0+\Delta y)-f(x_0,y_0)，$令$|M_0M|=\rho=\sqrt{(\Delta x)^2+(\Delta y)^2}，$如果$\lim\limits_{\rho\to0}\frac{\Delta z}{\rho}$存在，称此极限为$z=f(x,y)$在$M_0$处沿$l$的方向导数，记作$\frac{\partial z}{\partial l}|_{M_0}$ 定理设$z=f(x,y)$连续可偏导(可微)，$l$的方向角为$\alpha，\beta$，射线$l$的单位向量为$e=\{cos\alpha，cos\beta\}$，则 \frac{\partial z}{\partial l}|_{M_0}=\frac{\partial z}{\partial x}|_{M_0}\cdot cos\alpha+\frac{\partial z}{\partial y}|_{M_0}\cdot cos\beta几何意义在一元函数中，$A$点的导数是$A$点切线的斜率 对于二元函数$f(x,y)$在某个方向上也是有切线的，其切线的斜率就是方向导数 梯度$M_0$点不止一个方向，而是$360°$都有方向，每个方向都有方向导数，那么哪个方向上的方向导数最大呢？ \begin{equation}\begin{split}\frac{\partial z}{\partial l}|_{M_0}&=\frac{\partial z}{\partial x}|_{M_0}\cdot cos\alpha+\frac{\partial z}{\partial y}|_{M_0}\cdot cos\beta\\&=\{\frac{\partial z}{\partial x}，\frac{\partial z}{\partial y}\}_{M_0}\cdot\{cos\alpha，cos\beta\}\\&=|\{\frac{\partial z}{\partial x}，\frac{\partial z}{\partial y}\}_{M_0}|\cdot cos\theta\end{split}\end{equation}$\theta$为射线$l$与向量$\{\frac{\partial z}{\partial x}，\frac{\partial z}{\partial y}\}_{M_0}$间的夹角，$l$的方向是变化的，$\{\frac{\partial z}{\partial x}，\frac{\partial z}{\partial y}\}_{M_0}$的方向是固定的。很明显，当$\theta=0$，即$l$与$\{\frac{\partial z}{\partial x}，\frac{\partial z}{\partial y}\}_{M_0}$方向一致时，方向导数取最大值，函数增加最快。由此引出梯度的定义： 向量$(\frac{\partial z}{\partial x}，\frac{\partial z}{\partial y})^T$称为$z=f(x,y)$的梯度，记作grad $f(x,y)$或$\nabla\ f(x,y)$ 显然，梯度是一个矢量，其方向上的方向导数最大，其大小正好是此最大方向导数。当$\theta=\pi$，即$l$与梯度的方向相反时，方向导数取最小值，函数减少最快。 梯度下降法有了以上的铺垫，就可以更好地理解梯度下降法(Gradient Descent)了。 基本思想我们有一个可微的函数，想象这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向往下走，对应到函数中，就是找到当前点的梯度，然后朝着梯度相反的方向，就能让函数值下降的最快。我们重复利用这个方法，反复求取梯度，最后就能到达局部最小值。 从上图可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。如果目标函数是凸函数，梯度下降法得到的解就一定是全局最优解。还有就是初始点选的不同，结果也不一定相同。 算法过程 \theta^t=\theta^{t-1}-\alpha\nabla_{\theta}\ J(\theta)其中$J$是关于$\theta$的函数，假设当前处于$\theta_0$这一点，要从这个点走到$J$的最小值点。首先我们要确定前进的方向，也就是梯度的反方向，然后走一段距离的步长，也就是α，走完这段步长，就到达了另一个点。式中的$\alpha$称为学习率或者步长，用来控制梯度下降时迈出多大的步子(恒$＞0$)。如果$\alpha$过小，则下降速度会很慢，如果过大，则可能越过最低点，甚至可能无法收敛。通常$\alpha$大致按$3$的倍数取，如$\alpha=\cdots,0.001,0.003,0.01,0.03,0.1,0.3,1,\cdots$ 实例单变量函数的梯度下降假设有一个单变量的函数$J(\theta)=\theta^2$，设初始点为$\theta^0=1$，$\alpha=0.4$，梯度下降的迭代计算过程： 经过四次的运算，也就是走了四步，基本就抵达了函数的最低点 多变量函数的梯度下降假设目标函数为$J(\theta)=\theta_1^2+\theta_2^2$，设初始点为$\theta^0=(1,3)$，学习率为$\alpha=0.1$，函数的梯度为$\nabla\ f(\theta)=(2\theta_1,2\theta_2)$，跌代过程： 已经基本靠近函数的最小值点]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令总结]]></title>
    <url>%2F2018%2F12%2F24%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录相关命令查看目录内容ls是英文单词 list的简写，其功能为列出目录的内容。 ls常用选项: 参数 含义 -a 显示指定目录下所有子目录与文件，包括隐藏文件 -l 以列表方式显示文件的详细信息 -h 配合 -l 以人性化的方式显示文件大小 -d 配合 -l 查询某个目录的详细信息 ls通配符的使用: 通配符 含义 * 任意个数个字符 ? 任意一个字符,至少一个 [] 可以匹配字符组中的任一一个 [abc] 匹配 a、b、c 中的任意一个 [a-f] 匹配从 a 到 f 范围内的的任意一个字符 切换目录cd是英文单词 change directory 的简写，其功能为更改当前的工作目录 命令 含义 cd(cd ~) 切换到当前用户的主目录(/home/用户名) cd . 保持在当前目录不变 cd .. 切换到上级目录 cd - 在最近两次工作目录之间来回切换 文件相关命令创建和删除操作touch 创建文件或修改文件时间 如果文件 不存在，可以创建一个空白文件 如果文件 已经存在，可以修改文件的末次修改日期 mkdir创建一个新的目录 选项 含义 -p 递归创建目录 如:mkdir -p a/b/c 新建目录的名称不能与当前目录中已有的目录或文件同名 rm删除文件或目录 选项 含义 -f 强制删除，忽略不存在的文件，无需提示 -r 递归地删除目录下的内容，删除文件夹时必须加此参数 用rm删除文件后不能恢复 拷贝和移动文件 命令 作用 tree [目录名] 以树状图列出文件目录结构 cp 源文件 目标文件 复制文件或者目录 mv 源文件 目标文件 移动文件或者目录／文件或者目录重命名 tree 选项 含义 -d 只显示目录 cp 选项 含义 -i 覆盖文件前提示 -r 若给出的源文件是目录文件，则 cp 将递归复制该目录下的所有子目录和文件，目标文件必须为一个目录名 查看文件内容catcat命令用来 查看文件内容、创建文件、文件合并、追加文件内容 等功能，cat 会一次显示所有的内容，适合查看内容较少的文本文件 选项 含义 -b 对非空输出进行行编号 -n 对输出的所有进行行编号 tac可以反向查看文件内容 moremore命令可以用于分屏显示文件内容，每次只显示一页内容，适合于查看内容较多的文本文件 操作键 空格键或f 翻页 回车 换行 b 回滚一页 q或Q 退出 /关键词 搜索关键词 less和more有相同的功能，但是less可以用PgUp键往前翻一页，向上的箭头回滚一行，向下的箭头往下换行，n进行翻页 grep grep 是一个强大的文本搜索命令，允许对文本文件进行模式查找，即通过正则表达式查找 选项 含义 -n 显示匹配行及行号 -v 显示不包含匹配文本的所有行（相当于求反） -i 忽略大小写 参数 含义 ^a 搜索以 a 开头的行 b$ s搜索以b结尾的行 链接命令软链接 命令：ln -s 被链接的源文件 链接文件 作用：建立文件的软链接，类似于Windows下的快捷方式 注意： 源文件要使用绝对路径，不能使用相对路径，这样可以方便移动链接文件后，仍然能够正常使用 没有 -s 选项建立的是一个 硬链接文件 文件软硬链接的示意图 在 Linux 中，文件名和文件的数据是分开存储的，软链接存放的是链接文件的完整路径，当链接的源文件删除时，就不能通过软链接访问文件的数据。硬链接相当于文件的另一个文件名，文件名删除时，仍能通过硬链接访问文件数据，只有把一个文件的所有文件名，即所有硬链接删除之后，文件数据才会从磁盘中删除。 其他echo echo 会在终端中显示参数指定的文字，通常和重定向联合使用 重定向 &gt; 和 &gt;&gt; 将命令本应显示在终端上的内容输出或者追加到指定文件中，&gt;表示输出，会覆盖原文件的内容，&gt;&gt;表示重定向，会将内容追加到指定文件的末尾。 管道 | 管道可以将一个命令的输出作为另一个命令的输入，常用的管道命令有：more和grep]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT线性代数公开课笔记(五)——向量空间、列空间和零空间]]></title>
    <url>%2F2018%2F11%2F18%2FMIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0(%E4%BA%94)%E2%80%94%E2%80%94%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E3%80%81%E5%88%97%E7%A9%BA%E9%97%B4%E5%92%8C%E9%9B%B6%E7%A9%BA%E9%97%B4%2F</url>
    <content type="text"><![CDATA[向量空间①所有向量空间都必须包含零向量，即包含原点。 ②向量空间中任意向量的数乘、求和运算得到的向量也在该空间中，即向量空间要满足加法封闭和数乘封闭。 ③向量空间$R^n$包含所有的n维向量，分量均为实数。 子空间向量空间的子空间也必须满足加法封闭和数乘封闭，并且也包含零向量。 $R^2$的子空间：①$R^2本身$；②过原点的直线；③零向量(即原点)； $R^3$的子空间：①$R^3本身$；②过原点的平面；③过原点的直线；④零向量。 列空间设$A=\begin{bmatrix}1&amp;3\\2&amp;3\\4&amp;1\end{bmatrix}$，其中各列属于$R^3$，那么所有列的所有线性组合构成$R^3$的一个子空间，在这里为过原点的一个平面，称该子空间为$A$的列空间，记作$C(A)$。如果$A$的两列共线，则列空间为一条直线。 构造矩阵列空间的方法：取出各列，然后线性组合，则所有的线性组合构成列空间。 假设$P$为三维空间中过原点的平面，$L$为过原点的直线($L$不在$P$内)，$P、L$都为子空间，而$P∪L$不是子空间，因为加法不封闭，$P∩L$是子空间，因为只含零向量。一般情况下，若$S、T$均为子空间，则$S∩T$也为子空间。 $Ax=b$并不是对任意的$b$都有解，只有$b$属于$A$的列空间时才有解。例如： Ax=\begin{bmatrix}1&1&2\\2&1&3\\3&1&4\\4&1&5\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}b_1\\b_2\\b_3\\b_4\end{bmatrix}$A$的三个列向量的线性组合无法充满整个四维空间，所以可能有些$b$不是这三个列向量的线性组合。 在$A$中，其实第三列可以去掉，是前两列的线性组合，对结果没有影响。 零空间$A$的零空间为$Ax=0$中所有的解$x$组成的集合，记作$N(A)$。不管$A$是什么矩阵，其零空间必然含有零向量。例如： Ax=\begin{bmatrix}1&1&2\\2&1&3\\3&1&4\\4&1&5\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix}$\begin{bmatrix}0\\0\\0\end{bmatrix}、\begin{bmatrix}1\\1 \\ -1\end{bmatrix}\cdots\begin{bmatrix}c\\c \\ -c\end{bmatrix}$，即$c\begin{bmatrix}1\\1 \\ -1\end{bmatrix}$为$A$的零空间，为三维空间中过原点的直线。 验证：$A$的零空间为子空间。 证明如下：①如果$Ax=0并且Ay=0$，那么$A(ax+by)=aAx+bAy=0$，也就是说$v$和$w$都在零空间，那么其和$v$和$w$的线性组合也在零空间内。②如果$Av=0$，那么$A(av)=aAv=0$，即如果$v$在零空间，那么其数乘$av$也在零空间内。综上所述，零空间满足加法和数乘封闭，并且包含零向量，所以为子空间。 如果$Ax=\begin{bmatrix}1&amp;1&amp;2\\2&amp;1&amp;3\\3&amp;1&amp;4\\4&amp;1&amp;5\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}1\\2\\3\\4\end{bmatrix}$，其所有的解构成子空间吗？ 答案是否定的，因为解中不包含零向量，这里的解其实为三维空间中不过原点的直线。 构造子空间的两种方法： ①取各列的线性组合； ②从方程组中通过让$x$满足特定条件来得到子空间。]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT线性代数公开课笔记(四)——转置与置换]]></title>
    <url>%2F2018%2F11%2F16%2FMIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0(%E5%9B%9B)%E2%80%94%E2%80%94%E8%BD%AC%E7%BD%AE%E4%B8%8E%E7%BD%AE%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[转置设$A=\begin{bmatrix}1&amp;2&amp;4\\3&amp;3&amp;1\end{bmatrix}$，则$A^T=\begin{bmatrix}1&amp;3\\2&amp;3\\4&amp;1\end{bmatrix}$，即$(A^T)_{ij}=A_{ji}$，$A^TA=\begin{bmatrix}10&amp;11&amp;7\\11&amp;13&amp;11\\7&amp;11&amp;17\end{bmatrix}$，对于所有的矩阵$A$，$A^TA$都是对称的，对于对称矩阵，其转置等于其本身。 $AA^{-1}=I$，两边同时转置得，$(A^{-1})^TA^T=I$，所以有$(A^T)^{-1}=(A^{-1})^T$ 置换矩阵置换矩阵是用来完成行互换的矩阵，记作$P$，即单位矩阵的行重新排列后的矩阵，例如： 互换行一与行二的置换矩阵P_{12}=\begin{bmatrix}0&1&0\\1&0&0\\0&0&1\end{bmatrix}\\互换行一与行三的置换矩阵P_{13}=\begin{bmatrix}0&0&1\\0&1&0\\1&0&0\end{bmatrix}\\互换行二与行三的置换矩阵P_{23}=\begin{bmatrix}1&0&0\\0&0&1\\0&1&0\end{bmatrix}上面这两个矩阵都是行互换一次的置换矩阵，如果交换所有的行，可以得到下面的矩阵 \begin{bmatrix}0&1&0\\0&0&1\\1&0&0\end{bmatrix}、\begin{bmatrix}0&0&1\\1&0&0\\0&1&0\end{bmatrix}加上单位矩阵$I$本身($I$是不需做任何变换的矩阵，即任何矩阵乘以$I$等于其本身)，如果将这6个矩阵两两相乘，结果仍在这6个矩阵当中，它们的逆也在其中，我们称这6个矩阵构成一个群。 我们还可以得出一个结论：置换矩阵的逆等于其转置，即$P^{-1}=P^T$，并且所有置换矩阵均可逆。对于$n×n$的矩阵，总共有$n!$种置换矩阵。]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT线性代数公开课笔记(三)——矩阵乘法和逆]]></title>
    <url>%2F2018%2F11%2F14%2FMIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0(%E4%B8%89)%E2%80%94%E2%80%94%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E5%92%8C%E9%80%86%2F</url>
    <content type="text"><![CDATA[矩阵乘法法一： 假设$A$是$m×n$矩阵，$B$是$n×p$矩阵，$AB=C$，则$c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{in}b_{nj}=\sum_{k=1}^na_{ik}b_{kj}$ 法二： 将乘法考虑为矩阵乘以向量，将$B$看成$p$个单独的列向量，然后用$A$乘以每个列向量，得到$C$中的每一列。 法三： $A$的每一行乘以$B$得到$C$中的每一行，$C$中的各行是$B$中各行的线性组合。 法四： $AB=A$中各列与$B$各行的乘积之和$=$列一×行一+列二×行二+$\cdots$ 如$\begin{bmatrix}2&amp;7\\3&amp;8\\4&amp;9\end{bmatrix}\begin{bmatrix}1&amp;6\\0&amp;0\end{bmatrix}=\begin{bmatrix}2\\3\\4\end{bmatrix}\begin{bmatrix}1&amp;6\end{bmatrix}+\begin{bmatrix}7\\8\\9\end{bmatrix}\begin{bmatrix}0&amp;0\end{bmatrix}=\begin{bmatrix}2&amp;12\\3&amp;18\\4&amp;24\end{bmatrix}$ 法五： 分块乘法 矩阵的逆首先不是所有的矩阵都存在逆，假设$A$为方阵，并且逆存在，则有 A^{-1}A=AA^{-1}=I对于方阵而言，左逆等于右逆，但是如果$A$为非方阵，左逆就不等于右逆。通常我们称不可逆矩阵为奇异矩阵。下面看看不可逆的情况：$A=\begin{bmatrix}1&amp;3\\2&amp;6\end{bmatrix}$ 为什么$A$不可逆呢？首先$A$对应的行列式$|A|=0$，这个可以用于判断$n$阶方阵$A$是否可逆。其次，如果存在非零向量$x$，使$Ax=0$，则$A$不可逆。在这里取$x=\begin{bmatrix}-3\\1\end{bmatrix}$，有 Ax=\begin{bmatrix}1&3\\2&6\end{bmatrix}\begin{bmatrix}-3\\1\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}即奇异矩阵的各列通过线性组合能够得到零向量。 高斯-若尔当消元设$A=\begin{bmatrix}1&amp;3\\2&amp;7\end{bmatrix}$，那么如何求$A^{-1}$呢？按照逆的定义，有 \begin{bmatrix}1&3\\2&7\end{bmatrix}\begin{bmatrix}a&c\\b&d\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}按照矩阵乘法，有 \left\{ \begin{aligned} \begin{bmatrix}1&3\\2&7\end{bmatrix}\begin{bmatrix}a\\b\end{bmatrix}=\begin{bmatrix}1\\0\end{bmatrix}\\ \begin{bmatrix}1&3\\2&7\end{bmatrix}\begin{bmatrix}c\\d\end{bmatrix}=\begin{bmatrix}0\\1\end{bmatrix}\\ \end{aligned} \right.解此方程组即可。但是该方法比较麻烦，高斯-若尔当(Gauss-Jordan)方法可以一次处理所有的方程。构造这样的方程$\left[\begin{array}{cc|cc}1&amp;3&amp;1&amp;0\\2&amp;7&amp;0&amp;1\end{array}\right]$，左侧为矩阵$A$，右侧为$I$，接下来通过消元法，将左侧变为单位阵，右侧即为$A$的逆矩阵。具体过程如下： \left[\begin{array}{cc|cc}1&3&1&0\\2&7&0&1\end{array}\right]\underrightarrow{\text{row2-2row1}}\begin{bmatrix}1&3&1&0\\0&1&-2&1\end{bmatrix}\underrightarrow{\text{row1-3row2}}\left[\begin{array}{cc|cc}1&0&7&-3\\0&1&-2&1\end{array}\right]所以$A^{-1}=\begin{bmatrix}7&amp;-3 \\ -2&amp;1\end{bmatrix}$ 总结： 求$A^{-1}$的方法 \begin{bmatrix}A&I\end{bmatrix}\underrightarrow{\text{消元}}\begin{bmatrix}I&A^{-1}\end{bmatrix}]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT线性代数公开课笔记(二)——矩阵消元]]></title>
    <url>%2F2018%2F11%2F13%2FMIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0(%E4%BA%8C)%E2%80%94%E2%80%94%E7%9F%A9%E9%98%B5%E6%B6%88%E5%85%83%2F</url>
    <content type="text"><![CDATA[消元法有方程组$\left\{\begin{aligned}x+2y+z=2\\3x+8y+z=12\\4y+z=2\end{aligned}\right.$，写成矩阵形式$Ax=b$为$\begin{bmatrix}1&amp;2&amp;1\\3&amp;8&amp;1\\0&amp;4&amp;1\end{bmatrix}\begin{bmatrix}x\\y\\z\end{bmatrix}=\begin{bmatrix}2\\12\\2\end{bmatrix}$ 消元法的思路： $A=\begin{bmatrix}1&amp;2&amp;1\\3&amp;8&amp;1\\0&amp;4&amp;1\end{bmatrix}\underrightarrow{\text{row2-3row1}}\begin{bmatrix}1&amp;2&amp;1\\0&amp;2&amp;-2\\0&amp;4&amp;1\end{bmatrix}\underrightarrow{\text{row3-2row2}}\begin{bmatrix}1&amp;2&amp;1\\0&amp;2&amp;-2\\0&amp;0&amp;5\end{bmatrix}=U$，U的对角线上的元素为三个主元。首先，主元不能为零；其次，如果在消元时遇到主元位置为零，则需要交换行，使主元不为零。当消元失效时，将不能得到三个主元。 下面进行回代，将矩阵$\begin{bmatrix}A&amp;b\end{bmatrix}$称为增广矩阵。有$\begin{bmatrix}A&amp;b\end{bmatrix}=\begin{bmatrix}1&amp;2&amp;1&amp;2\\3&amp;8&amp;1&amp;12\\0&amp;4&amp;1&amp;2\end{bmatrix}\to\begin{bmatrix}1&amp;2&amp;1&amp;2\\0&amp;2&amp;-2&amp;6\\0&amp;4&amp;1&amp;2\end{bmatrix}\to\begin{bmatrix}1&amp;2&amp;1&amp;2\\0&amp;2&amp;-2&amp;6\\0&amp;0&amp;5&amp;-10\end{bmatrix}$，此时方程组变为$\left\{\begin{aligned}x+2y+z=2\\2y-2z=6\\5z=-10\end{aligned}\right.$，很容易解出$x=2,y=1,z=-2$ 消元矩阵下面介绍用行来计算矩阵乘法： \begin{bmatrix}1&2&7\end{bmatrix}\begin{bmatrix}\cdots&row_1&\cdots\\\cdots&row_2&\cdots\\\cdots&row_3&\cdots\end{bmatrix}=1×row_1+2×row_2+7×row_3从矩阵$\begin{bmatrix}1&amp;2&amp;1\\3&amp;8&amp;1\\0&amp;4&amp;1\end{bmatrix}$到矩阵$\begin{bmatrix}1&amp;2&amp;1\\0&amp;2&amp;-2\\0&amp;4&amp;1\end{bmatrix}$是第二行减去3倍的第一行，第一、三行不变，则有 \begin{bmatrix}1&0&0\\-3&1&0\\0&0&1\end{bmatrix}\begin{bmatrix}1&2&1\\3&8&1\\0&4&1\end{bmatrix}=\begin{bmatrix}1&2&1\\0&2&-2\\0&4&1\end{bmatrix}将消元矩阵$\begin{bmatrix}1&amp;0&amp;0\-3&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix}$记作$E_{21}​$，表示第二行第一个元素变为零。同理有 \begin{bmatrix}1&0&0\\0&1&0\\0&-2&1\end{bmatrix}\begin{bmatrix}1&2&1\\0&2&-2\\0&4&1\end{bmatrix}=\begin{bmatrix}1&2&1\\0&2&-2\\0&0&5\end{bmatrix}将消元矩阵$\begin{bmatrix}1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;-2&amp;1\end{bmatrix}$记作$E_{32}$，$E_{21}、E_{32}$都为初等矩阵。将上面两步综合起来有 E_{32}(E_{21}A)=U由于矩阵乘法满足结合律，又可以写成 (E_{32}E_{21})A=U下面介绍一种用于置换两行或两列的矩阵，称为置换矩阵，例如： \begin{bmatrix}0&1\\1&0\end{bmatrix}\begin{bmatrix}a&b\\c&d\end{bmatrix}=\begin{bmatrix}c&d\\a&b\end{bmatrix}如果交换两列则有 \begin{bmatrix}a&b\\c&d\end{bmatrix}\begin{bmatrix}0&1\\1&0\end{bmatrix}=\begin{bmatrix}b&a\\d&c\end{bmatrix}总结：在左边用矩阵做乘法进行的是行变换，在右边用矩阵做乘法进行的是列变换。即列变换时右乘，行变换时左乘。 逆通过消元可以将矩阵$A$变换为$U$，那么将$U$变回$A$的过程称为逆变换。在这里先简单提一下矩阵的逆，这里以$E_{21}$为例： $E_{21}$将$A$的第二行减去3倍的第一行，那么其逆变换为第二行加3倍的第一行，所以逆矩阵为$\begin{bmatrix}1&amp;0&amp;0\\3&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix}$，我们把$E$的逆记作$E^{-1}$，有$E^{-1}E=I$，有 \begin{bmatrix}1&0&0\\3&1&0\\0&0&1\end{bmatrix}\begin{bmatrix}1&0&0\\-3&1&0\\0&0&1\end{bmatrix}=\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT线性代数公开课笔记(一)——方程组的几何解释]]></title>
    <url>%2F2018%2F11%2F12%2FMIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0(%E4%B8%80)%E2%80%94%E2%80%94%E6%96%B9%E7%A8%8B%E7%BB%84%E7%9A%84%E5%87%A0%E4%BD%95%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[从一个例子讲起：2个方程，2个未知数的方程组 \left\{ \begin{aligned} 2x-y=0\\ -x+2y=3\\ \end{aligned} \right.写成矩阵形式为 \begin{bmatrix}2&-1\\-1&2\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}0\\3\end{bmatrix}第一个矩阵称为系数矩阵A，第二个矩阵称为x，第三个矩阵称为b，这样方程组可写为$Ax=b$，一个行图像显示一个方程，可以画出该方程组的行图像： ——方程组的几何解释/3.jpg) 列图像： 原方程组可写为$x\begin{bmatrix}2 \\ -1\end{bmatrix}+y\begin{bmatrix}-1\\2\end{bmatrix}=\begin{bmatrix}0\\3\end{bmatrix}$，该方程是寻找如何将$\begin{bmatrix}2 \\ -1\end{bmatrix}$和$\begin{bmatrix}-1\\2\end{bmatrix}$两个向量正确组合，来构成$\begin{bmatrix}0\\3\end{bmatrix}$，这就需要找到正确的线性组合。将$\begin{bmatrix}2 \\ -1\end{bmatrix}$记作$col_1$，$\begin{bmatrix}-1\\2\end{bmatrix}$记作$col_2$，当x=1,y=2时，等式成立。下面画出列向量： ——方程组的几何解释\4.png) 如果选取所有的x和y，即所有的组合，结果会得到整个二维空间。下面来看三维空间的例子： \left\{ \begin{aligned} 2x-y=0\\ -x+2y-z=-1\\ -3y+4z=4 \end{aligned} \right.在这里$A=\begin{bmatrix}2&amp;-1&amp;0 \\ -1&amp;2&amp;-1\\0&amp;-3&amp;4\end{bmatrix}，b=\begin{bmatrix}0 \\ -1\\4\end{bmatrix}$。在三维空间中，每一个方程确定一个平面，而例子中的三个平面会相交于一点，这个点就是方程组的解。将方程组写成列向量的线性组合： x\begin{bmatrix}2\\-1\\0\end{bmatrix}+y\begin{bmatrix}-1\\2\\3\end{bmatrix}+z\begin{bmatrix}0\\-1\\4\end{bmatrix}=\begin{bmatrix}0\\-1\\4\end{bmatrix}同样将这三个列向量分别称为$col_1、col_2、col_3$，显而易见，当$x=y=0,z=1$时满足该等式。但不是对于所有的右侧向量$b$都有解，当$col_1、col_2、col_3$共面时，只有当$b$在此平面内时，方程组才有解，否则无解。 最后介绍矩阵形式的$Ax=b$，举个例子，取$A=\begin{bmatrix}2&amp;5\\1&amp;3\end{bmatrix}，x=\begin{bmatrix}1\\2\end{bmatrix}$，则$Ax=\begin{bmatrix}2&amp;5\\1&amp;3\end{bmatrix}\begin{bmatrix}1\\2\end{bmatrix}=1\begin{bmatrix}2\\1\end{bmatrix}+2\begin{bmatrix}5\\3\end{bmatrix}=\begin{bmatrix}12\\7\end{bmatrix}$ 总之，$A$右侧乘以一个向量可以看成$A$的各列的线性组合。]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性模型]]></title>
    <url>%2F2018%2F10%2F29%2F%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[线性回归模型概述给定数据集D={$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots(x^{(m)},y^{(m)})$}，$x^{(i)}\in\mathcal{X}\subseteq\mathbb{R}^m,y^{(i)}\in\mathcal{Y}\subseteq\mathbb{R},i=1,2,\cdots,m,$其中$x^{(i)}=(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)})^T,$m为样本的数量，n为特征的数量。 线性模型试图学得一个通过属性的线性组合来进行预测的函数，即 f(x)=w_1x_1+w_2x_2+...+w_nx_n+b,其中$w$为权重，b为截距，写成向量形式为 f(x)=w^Tx+b为了简化公式，设$x^{(i)}=(x_1^{(i)},x_2^{(i)},…x_n^{(i)},1)^T，w=(w_1,w_2,…,w_n,b)^T$，简化之后为 f(x)=\sum_{i=0}^nw_ix_i=w^Tx我们的目标便是通过给定的数据集D来学习参数$w$，对于给定的样本$x^{(i)}$，其预测值$\hat{y}^{(i)}=w^Tx^{(i)}$，与真实值$y^{(i)}$越接近越好。这里我们采用平方损失函数，则在训练集D上，模型的损失函数为 L(w)=\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2 \\\quad\quad=\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2这样，我们的目标便变为损失函数最小化。为了之后求导方便，在损失函数前乘以1/2，即： w^*=\mathop{\arg\min}_{w}\frac{1}{2}\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2为了求出使$L(w)$最小的$w$值，我们可以使用梯度下降法和正规方程两种方法。 梯度下降法梯度下降的思想是：开始时随机选择一个参数的组合$(w_0,w_1,w_2,…,w_n)$，计算损失函数，然后寻找下一个能让损失函数下降最多的参数组合，持续这么做直到一个局部最小值。通常选择不同的初始参数组合，可能会找到不同的局部最小值。 梯度下降算法公式为： 重复直到收敛{ ​ $w_j:=w_j-\alpha\frac{\partial}{\partial w_j}L(w)$ } 要实现这个算法，关键在于求出损失函数关于$w$的导数 $\frac{\partial}{\partial w_j}L(w)=\frac{\partial}{w_j}\frac{1}{2}\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})^2\\\quad\quad\quad\,\,\,\,=\frac{\partial}{\partial w_j}\frac{1}{2}\sum_{i=1}^m(w_0x_0^{(i)}+w_1x_1^{(i)}+…+w_jx_j^{(i)}+…+w_nx_n^{(i)}-y^{(i)})^2\\\quad\quad\quad\,\,\,\,=2·\frac{1}{2}\sum_{i=1}^m(w_0x_0^{(i)}+w_1x_1^{(i)}+…+w_nx_n^{(i)}-y^{(i)})·x_j^{(i)}\\\quad\quad\quad\,\,\,\,=\sum_{i=1}^m(w^Tx^{(i)}-y^{(i)})x_j^{(i)}$ 重复直到收敛{ ​ $w_j:=w_j+\alpha\sum_{i=1}^m(y^{(i)}-w^Tx^{(i)})x_j^{(i)}​$ (for every j) } 正规方程正规方程通过求解下面的方程来找出使损失函数最小的参数：$\frac{\partial}{\partial w}L(w)=0$ 矩阵导数假设函数$f:R^{m×n}\to R$，从m*n大小的矩阵映射到实数域，那么当矩阵为A时导函数定义如下所示： \frac{\partial f(A)}{\partial A}=\begin{bmatrix}\frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1n}}\\\vdots &\ddots&\vdots\\\frac{\partial f}{\partial A_{m1}}&\cdots&\frac{\partial f}{\partial A_{mn}} \end{bmatrix}例如A=$\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22} \end{bmatrix}$是2*2矩阵，给定函数$f:R^{2×2} \to R$为： f(A)=\frac{3}{2}A_{11}+5A_{12}^2+A_{21}A_{22}那么$\frac{\partial f(A)}{\partial A}=\begin{bmatrix}\frac{3}{2}&amp;10A_{12}\\A_{22}&amp;A_{21} \end{bmatrix}$，我们还要引入矩阵的迹(trace)，简写为tr。对于一个给定的n*n的方阵A，它的迹定义为对角线元素之和： tr\ A=\sum_{i=1}^nA_{ii}如果有两矩阵A和B，满足AB为方阵，则迹运算有以下性质： trAB=trBA\\trABC=trCAB=trBCA\\trA=trA^T\\tr(A+B)=trA+trB\\tr\ aA=atrA接下来提出一些矩阵导数： \frac{\partial(trAB)}{\partial A}=B^T\\\frac{\partial f(A)}{\partial A^T}=(\frac{\partial f(A)}{\partial A})^T\\\frac{\partial(trABA^TC)}{\partial A}=CAB+C^TAB^T\\\frac{\partial |A|}{\partial A}=|A|(A^{-1})^T下面把损失函数$L(w)$用向量的形式表述。令 X=\begin{bmatrix}(x^{(1)})^T\\(x^{(2)})^T\\\vdots\\(x^{(m)})^T\end{bmatrix}=\begin{bmatrix}x_1^{(1)}&x_2^{(1)}&\cdots& x_n^{(1)}&1\\x_1^{(2)}&x_2^{(2)}&\cdots&x_n^{(2)}&1\\\vdots&\vdots&\ddots&\vdots&1\\x_1^{(m)}&x_2^{(m)}&\cdots&x_n^{(m)}&1\end{bmatrix}\\y=\begin{bmatrix}y^{(1)}\\y^{(2)}\\\vdots\\y^{(m)}\end{bmatrix},w=\begin{bmatrix}w_1\\w_2\\\vdots\\w_n\\b\end{bmatrix}则有 Xw-y=\begin{bmatrix}f(x^{(1)})-y^{(1)}\\f(x^{(2)})-y^{(2)}\\\vdots\\f(x^{(m)})-y^{(m)}\end{bmatrix}\\\frac{1}{2}(Xw-y)^T(Xw-y)=\frac{1}{2}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2=L(w) \begin{equation} \begin{split} \frac{\partial L(w)}{\partial w}&=\frac{\partial }{\partial w}\frac{1}{2}(Xw-y)^T(Xw-y)\\ &=\frac{1}{2}\frac{\partial}{\partial w}(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty)\\ &=\frac{1}{2}\frac{\partial}{\partial w}tr(w^TX^TXw-w^TX^Ty-y^TXw+y^Ty)\\ &=\frac{1}{2}\frac{\partial}{\partial w}(tr(w^TX^TXw)-2tr(y^TXw))\\ &=\frac{1}{2}(X^TXw+X^TXw-2X^Ty)\\ &=X^TXw-X^Ty \end{split} \end{equation}令其等于0便得到下面的正规方程： X^TXw=X^Ty当$X^TX$可逆时，可得： w^*=(X^TX)^{-1}X^Ty于是学得的线性回归模型为： f(x^{(i)})=w^{*T}x^{(i)}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻]]></title>
    <url>%2F2018%2F10%2F16%2Fk-%E8%BF%91%E9%82%BB%2F</url>
    <content type="text"><![CDATA[概述k近邻法是一种基本的分类与回归算法，它的输入为实例的特征向量，通过计算新数据与训练数据特征值之间的距离，然后选取与该实例最邻近的k个实例，这k个实例的多数属于哪一类，就把该输入实例分到这个类。对于分类问题，输出为实例的类别，对于新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测；对于回归问题，输出为实例的值，对于新的实例，取其k个最近邻的训练实例的平均值作为预测值。 kNN三要素k近邻法不具有显式的学习过程，它是直接预测，实际上是利用训练数据集对特征向量空间划分，作为其分类的模型，模型由距离度量、k值的选择、分类决策规则三要素决定。 距离度量特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是n维实数向量空间$\mathbb{R}^n$。k近邻模型的特征空间的距离一般使用欧氏距离，也可以是更一般的$L_p$距离。 设特征空间$\mathcal{X}$是n维实数向量空间，$x_i,x_j\in\mathcal{X},x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},\cdots,x_j^{(n)})^T,x_i,x_j$的$L_p$距离定义为： L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p}这里$p\ge1$，当$p=2$时，为欧氏距离： L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^\frac{1}{2}当$p=1$时，为曼哈顿距离： L_1(x_i,x_j)=\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|当$p=\infty$时，为各个维度距离的最大值： L_\infty(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|k值的选择如果选择较小的k值，相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。即k值的减小意味着整体模型变得复杂，容易发生过拟合。 如果选择较大的k值，相当于用较大的邻域中的训练实例进行预测，其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误，即k值的增大意味着整体模型变得简单。当k=N时，无论输入实例是什么，都将简单地预测它为训练实例中最多的类。此时，模型过于简单，完全忽略训练实例中的大量有用信息。 在应用中，k一般取一个较小的数值。通常采用交叉验证法选取最优的k值，就是比较不同k值时的交叉验证平均误差率，选择误差率最小的那个k值。 分类决策规则k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个近邻的训练实例中的多数类决定输入实例的类，也可以基于距离的远近进行加权投票，距离越近的样本权重越大。 如果分类的损失函数为0-1损失函数，分类函数为 f：\mathbb{R}^n\to\{c_1,c_2,\cdots,c_K\}那么误分类的概率为 P(Y\ne f(X))=1-P(Y=f(X))对给定的实例$x\in\mathcal{X}$，其最近邻的k个训练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是 \frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i\ne c_j)=1-\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i=c_j)要使误分类率最小即经验风险最小，就要使$\sum_\limits{x_i\in N_k(x)}I(y_i=c_j)$最大，所以多数表决规则等价于经验风险最化。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>kNN</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
